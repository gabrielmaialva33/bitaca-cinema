"""
Bitaca Cinema - Gemini Agent
Specialized agent using Google Gemini for complex reasoning and search
"""

from typing import Dict, Any, Optional
import asyncio

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from gemini_integration import get_gemini_client, GeminiIntegration


class GeminiAgent:
    """
    Specialized agent powered by Google Gemini
    Features deep thinking mode and Google Search integration
    """

    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize Gemini agent

        Args:
            api_key: Google AI API key (optional, uses env if not provided)
        """
        try:
            self.gemini = get_gemini_client() if not api_key else GeminiIntegration(api_key)
            self.enabled = True
            print("‚úÖ GeminiAgent initialized successfully")
        except Exception as e:
            print(f"‚ö†Ô∏è GeminiAgent initialization failed: {e}")
            self.gemini = None
            self.enabled = False

    async def process_query(
        self,
        query: str,
        context: Optional[Dict[str, Any]] = None,
        use_thinking: bool = False,
        use_search: bool = False
    ) -> str:
        """
        Process query with Gemini

        Args:
            query: User query
            context: Additional context
            use_thinking: Enable deep thinking mode
            use_search: Enable Google Search

        Returns:
            Generated response
        """
        if not self.enabled or not self.gemini:
            return "Gemini agent n√£o est√° dispon√≠vel no momento."

        try:
            # Build context prompt
            context_text = ""
            if context:
                if context.get('productions'):
                    context_text += "\n\nProdu√ß√µes relevantes:\n"
                    for prod in context['productions'][:3]:
                        context_text += f"- {prod.get('titulo', 'Unknown')}: {prod.get('sinopse', '')[:100]}...\n"

                if context.get('history'):
                    context_text += "\n\nHist√≥rico da conversa:\n"
                    for msg in context['history'][-3:]:
                        context_text += f"{msg['role']}: {msg['content'][:100]}...\n"

            # Enhance prompt with Bitaca context
            enhanced_prompt = f"""
            Voc√™ √© um assistente do Bitaca Cinema em Cap√£o Bonito/SP.

            Contexto do projeto:
            - Espa√ßo cultural underground e democr√°tico
            - 23 produ√ß√µes audiovisuais financiadas pela Lei Paulo Gustavo
            - Eixos tem√°ticos: Patrim√¥nio & Mem√≥ria, Cultura Musical, Meio Ambiente
            - Localiza√ß√£o: Galeria Bitaca Caf√© Bar

            {context_text}

            Query do usu√°rio: {query}

            Responda em portugu√™s brasileiro, sendo acolhedor e informativo.
            """

            # Decide which model and features to use
            if use_thinking or "complex" in query.lower() or "analise" in query.lower():
                # Use Pro model with thinking for complex queries
                response = await self.gemini.generate_with_thinking(
                    prompt=enhanced_prompt,
                    search=use_search
                )
            else:
                # Use Flash for simple queries
                response = await self.gemini.generate_content(
                    prompt=enhanced_prompt,
                    model="flash",
                    thinking_enabled=False,
                    search_enabled=use_search,
                    temperature=0.7
                )

            return response.get("response", "N√£o consegui gerar uma resposta.")

        except Exception as e:
            print(f"‚ùå GeminiAgent error: {e}")
            return f"Erro ao processar com Gemini: {str(e)}"

    async def analyze_cultural_impact(
        self,
        production_data: Dict[str, Any]
    ) -> str:
        """
        Analyze cultural impact of a production using Gemini's reasoning

        Args:
            production_data: Production information

        Returns:
            Cultural analysis
        """
        if not self.enabled or not self.gemini:
            return "An√°lise n√£o dispon√≠vel."

        prompt = f"""
        Analise o impacto cultural desta produ√ß√£o de Cap√£o Bonito:

        T√≠tulo: {production_data.get('titulo')}
        Diretor: {production_data.get('diretor')}
        Eixo Tem√°tico: {production_data.get('eixo')}
        Sinopse: {production_data.get('sinopse')}

        Considere:
        1. Relev√¢ncia para a identidade local
        2. Preserva√ß√£o da mem√≥ria cultural
        3. Contribui√ß√£o para o audiovisual paulista
        4. Alinhamento com as pol√≠ticas culturais (Lei Paulo Gustavo)
        5. Potencial educativo e social

        Use racioc√≠nio profundo para conectar a obra ao contexto cultural brasileiro.
        """

        response = await self.gemini.generate_with_thinking(
            prompt=prompt,
            search=True  # Enable search for cultural context
        )

        return response.get("response", "An√°lise n√£o dispon√≠vel.")

    async def search_and_recommend(
        self,
        query: str,
        productions: list
    ) -> Dict[str, Any]:
        """
        Search for information and recommend productions

        Args:
            query: Search query
            productions: Available productions

        Returns:
            Recommendations with reasoning
        """
        if not self.enabled or not self.gemini:
            return {
                "recommendations": [],
                "reasoning": "Servi√ßo n√£o dispon√≠vel"
            }

        # Build productions context
        prod_context = "\n".join([
            f"- {p.get('titulo')}: {p.get('sinopse', '')[:100]}..."
            for p in productions[:10]
        ])

        prompt = f"""
        Com base na busca do usu√°rio e nas produ√ß√µes dispon√≠veis do Bitaca Cinema,
        fa√ßa recomenda√ß√µes personalizadas.

        Busca do usu√°rio: {query}

        Produ√ß√µes dispon√≠veis:
        {prod_context}

        Use o Google Search se necess√°rio para entender melhor o contexto da busca.
        Recomende 3-5 produ√ß√µes explicando o porqu√™ de cada escolha.
        Considere temas, estilos e relev√¢ncia cultural.
        """

        response = await self.gemini.generate_content(
            prompt=prompt,
            model="pro",
            thinking_enabled=True,
            search_enabled=True,
            temperature=0.6
        )

        # Parse response to extract recommendations
        text = response.get("response", "")

        return {
            "recommendations": text,
            "reasoning": response.get("thinking", ""),
            "search_used": True
        }

    async def compare_with_nvidia(
        self,
        query: str,
        nvidia_response: str
    ) -> str:
        """
        Compare/enhance NVIDIA response using Gemini

        Args:
            query: Original query
            nvidia_response: Response from NVIDIA model

        Returns:
            Enhanced or validated response
        """
        if not self.enabled or not self.gemini:
            return nvidia_response  # Fallback to original

        prompt = f"""
        Revise e melhore esta resposta sobre o Bitaca Cinema:

        Query original: {query}

        Resposta atual: {nvidia_response}

        Se necess√°rio:
        - Corrija informa√ß√µes incorretas
        - Adicione contexto relevante sobre Cap√£o Bonito
        - Torne mais acolhedor e cultural
        - Mantenha conciso (2-3 par√°grafos)

        Use seu conhecimento e busca se necess√°rio.
        """

        response = await self.gemini.generate_content(
            prompt=prompt,
            model="flash",
            search_enabled=True,
            temperature=0.5
        )

        return response.get("response", nvidia_response)

    def get_agent_info(self) -> Dict[str, Any]:
        """Get agent information"""
        return {
            "name": "GeminiAgent",
            "provider": "Google AI",
            "enabled": self.enabled,
            "models": [
                "gemini-2.0-flash-exp",
                "gemini-2.0-pro-exp",
                "gemini-2.0-flash-thinking-exp"
            ],
            "capabilities": [
                "Deep thinking mode",
                "Google Search integration",
                "Complex reasoning",
                "Real-time information",
                "Cultural analysis"
            ],
            "specialization": "Complex reasoning and web search"
        }

    async def stream_response(
        self,
        query: str,
        model: str = "flash",
        thinking: bool = False
    ):
        """
        Stream response for real-time output

        Args:
            query: User query
            model: Gemini model to use
            thinking: Enable thinking mode

        Yields:
            Response chunks
        """
        if not self.enabled or not self.gemini:
            yield "Gemini streaming n√£o dispon√≠vel."
            return

        try:
            async for chunk in self.gemini._stream_response(
                model_name=self.gemini.models[model],
                contents=[{
                    "role": "user",
                    "parts": [{"text": query}]
                }],
                config={
                    "temperature": 0.7,
                    "max_output_tokens": 1000,
                    "thinking_enabled": thinking
                }
            ):
                yield chunk

        except Exception as e:
            yield f"Erro no streaming: {str(e)}"


# Test function
async def test_gemini_agent():
    """Test Gemini agent"""
    print("\nüß™ Testing GeminiAgent...")

    agent = GeminiAgent()

    if not agent.enabled:
        print("‚ùå GeminiAgent not available (no API key)")
        return

    # Test basic query
    print("\n1. Testing basic query...")
    response = await agent.process_query(
        query="O que √© o Bitaca Cinema?",
        use_thinking=False
    )
    print(f"Response: {response[:200]}...")

    # Test with thinking
    print("\n2. Testing with thinking mode...")
    response = await agent.process_query(
        query="Analise a import√¢ncia cultural das produ√ß√µes de Cap√£o Bonito",
        use_thinking=True
    )
    print(f"Response: {response[:200]}...")

    # Test cultural analysis
    print("\n3. Testing cultural impact analysis...")
    production = {
        "titulo": "Ponteia Viola",
        "diretor": "Margarida Chaves",
        "eixo": "Patrim√¥nio & Mem√≥ria",
        "sinopse": "Document√°rio sobre a tradi√ß√£o da viola caipira"
    }
    analysis = await agent.analyze_cultural_impact(production)
    print(f"Analysis: {analysis[:200]}...")

    print("\n‚úÖ GeminiAgent test complete!")


if __name__ == "__main__":
    asyncio.run(test_gemini_agent())